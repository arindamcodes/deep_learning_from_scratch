{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will implement a simple RNN cell\n",
    "# Then will train a very simple language model with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f9b99ddda50>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will restrict my notation according to Andrew Ng's DL lectures notation\n",
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.W_hh = torch.nn.Parameter(torch.randn((hidden_size, hidden_size)))\n",
    "        self.W_xh = torch.nn.Parameter(torch.randn((input_size, hidden_size)))\n",
    "        self.b_h = torch.nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.zeros(x.shape[0], self.W_hh.shape[1])\n",
    "        for t in range(x.shape[1]):\n",
    "            h_new = torch.tanh(torch.matmul(h, self.W_hh) + torch.matmul(x[:, t], self.W_xh) + self.b_h)\n",
    "            h = h_new\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cell = RNN(24, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((24, 32))\n",
    "a = rnn_cell.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 32])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets build a very simple LM model\n",
    "class LM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, input_size, hidden_size):\n",
    "        super(LM, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, input_size)\n",
    "        self.rnn = RNN(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_emb = self.embeddings(x)\n",
    "        h = self.rnn(x_emb)\n",
    "        logits = self.fc(h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[the tragedie of hamlet by william shakespeare 1599]\n",
      "\n",
      "\n",
      "actus primus. scoena prima.\n",
      "\n",
      "enter barnardo a\n"
     ]
    }
   ],
   "source": [
    "# We will use very simple dataset to train a LM\n",
    "from nltk.corpus import gutenberg\n",
    "shakespeare_works = gutenberg.fileids()\n",
    "data = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "data = data.lower()\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(data))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "def encoding(chars):\n",
    "    chr_to_idx = {ch:i for i, ch in enumerate(chars)}\n",
    "    return chr_to_idx\n",
    "\n",
    "def decoding(chars):\n",
    "    idx_to_chr = {i:ch for i, ch in enumerate(chars)}\n",
    "    return idx_to_chr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr_to_idx = encoding(chars)\n",
    "idx_to_chr = decoding(chars)\n",
    "data_idx = [chr_to_idx[ch] for ch in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(data_idx))\n",
    "valid_size = int(0.1 * len(data_idx))\n",
    "train_data = data_idx[:train_size]\n",
    "valid_data = data_idx[train_size:train_size+valid_size]\n",
    "test_data = data_idx[train_size+valid_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_pairs(data, seq_len):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(0, len(data) - seq_len, seq_len):\n",
    "        inputs.append(data[i: i+seq_len])\n",
    "        labels.append(data[i+seq_len: i+seq_len+1])\n",
    "\n",
    "    x = torch.tensor(inputs, dtype=torch.long)\n",
    "    y = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 16\n",
    "hidden_size = 16\n",
    "seq_length = 8\n",
    "learning_rate = 0.01\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = create_data_pairs(train_data, seq_length)\n",
    "valid_x, valid_y = create_data_pairs(valid_data, seq_length)\n",
    "test_x, test_y = create_data_pairs(test_data, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_dataset = TensorDataset(valid_x, valid_y)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LM(vocab_size, input_size, hidden_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    \n",
    "    for inputs, targets in data_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        #print(\"inputs\", inputs.shape)\n",
    "        #print(\"targets\", targets.shape)\n",
    "        # Reset the gradients\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        #print(\"outputs\", outputs.shape)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion):\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    return val_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 2.8061, Validation Loss: 2.5604\n",
      "Epoch [2/100], Train Loss: 2.4393, Validation Loss: 2.4223\n",
      "Epoch [3/100], Train Loss: 2.3408, Validation Loss: 2.3973\n",
      "Epoch [4/100], Train Loss: 2.2892, Validation Loss: 2.3636\n",
      "Epoch [5/100], Train Loss: 2.2538, Validation Loss: 2.3337\n",
      "Epoch [6/100], Train Loss: 2.2381, Validation Loss: 2.3398\n",
      "Epoch [7/100], Train Loss: 2.2294, Validation Loss: 2.3000\n",
      "Epoch [8/100], Train Loss: 2.2122, Validation Loss: 2.2927\n",
      "Epoch [9/100], Train Loss: 2.2028, Validation Loss: 2.2907\n",
      "Epoch [10/100], Train Loss: 2.2030, Validation Loss: 2.3024\n",
      "Epoch [11/100], Train Loss: 2.1983, Validation Loss: 2.2830\n",
      "Epoch [12/100], Train Loss: 2.1949, Validation Loss: 2.2829\n",
      "Epoch [13/100], Train Loss: 2.1909, Validation Loss: 2.2927\n",
      "Epoch [14/100], Train Loss: 2.1767, Validation Loss: 2.2880\n",
      "Epoch [15/100], Train Loss: 2.1816, Validation Loss: 2.2886\n",
      "Epoch [16/100], Train Loss: 2.1735, Validation Loss: 2.2944\n",
      "Epoch [17/100], Train Loss: 2.1732, Validation Loss: 2.2918\n",
      "Epoch [18/100], Train Loss: 2.1700, Validation Loss: 2.2814\n",
      "Epoch [19/100], Train Loss: 2.1668, Validation Loss: 2.2866\n",
      "Epoch [20/100], Train Loss: 2.1690, Validation Loss: 2.2755\n",
      "Epoch [21/100], Train Loss: 2.1584, Validation Loss: 2.2724\n",
      "Epoch [22/100], Train Loss: 2.1588, Validation Loss: 2.2669\n",
      "Epoch [23/100], Train Loss: 2.1710, Validation Loss: 2.2664\n",
      "Epoch [24/100], Train Loss: 2.1831, Validation Loss: 2.2821\n",
      "Epoch [25/100], Train Loss: 2.1816, Validation Loss: 2.2556\n",
      "Epoch [26/100], Train Loss: 2.1685, Validation Loss: 2.2675\n",
      "Epoch [27/100], Train Loss: 2.1636, Validation Loss: 2.2703\n",
      "Epoch [28/100], Train Loss: 2.1646, Validation Loss: 2.2762\n",
      "Epoch [29/100], Train Loss: 2.1643, Validation Loss: 2.2718\n",
      "Epoch [30/100], Train Loss: 2.1582, Validation Loss: 2.2927\n",
      "Epoch [31/100], Train Loss: 2.1546, Validation Loss: 2.2750\n",
      "Epoch [32/100], Train Loss: 2.1673, Validation Loss: 2.2622\n",
      "Epoch [33/100], Train Loss: 2.1483, Validation Loss: 2.2564\n",
      "Epoch [34/100], Train Loss: 2.1435, Validation Loss: 2.2548\n",
      "Epoch [35/100], Train Loss: 2.1559, Validation Loss: 2.3004\n",
      "Epoch [36/100], Train Loss: 2.1614, Validation Loss: 2.2664\n",
      "Epoch [37/100], Train Loss: 2.1511, Validation Loss: 2.2490\n",
      "Epoch [38/100], Train Loss: 2.1549, Validation Loss: 2.2628\n",
      "Epoch [39/100], Train Loss: 2.1593, Validation Loss: 2.2930\n",
      "Epoch [40/100], Train Loss: 2.1644, Validation Loss: 2.2775\n",
      "Epoch [41/100], Train Loss: 2.1559, Validation Loss: 2.2769\n",
      "Epoch [42/100], Train Loss: 2.1642, Validation Loss: 2.2805\n",
      "Epoch [43/100], Train Loss: 2.1513, Validation Loss: 2.2829\n",
      "Epoch [44/100], Train Loss: 2.1592, Validation Loss: 2.2608\n",
      "Epoch [45/100], Train Loss: 2.1523, Validation Loss: 2.2611\n",
      "Epoch [46/100], Train Loss: 2.1524, Validation Loss: 2.2569\n",
      "Epoch [47/100], Train Loss: 2.1455, Validation Loss: 2.2569\n",
      "Epoch [48/100], Train Loss: 2.1494, Validation Loss: 2.2464\n",
      "Epoch [49/100], Train Loss: 2.1454, Validation Loss: 2.2639\n",
      "Epoch [50/100], Train Loss: 2.1538, Validation Loss: 2.2636\n",
      "Epoch [51/100], Train Loss: 2.1609, Validation Loss: 2.2637\n",
      "Epoch [52/100], Train Loss: 2.1439, Validation Loss: 2.2514\n",
      "Epoch [53/100], Train Loss: 2.1398, Validation Loss: 2.2678\n",
      "Epoch [54/100], Train Loss: 2.1522, Validation Loss: 2.2589\n",
      "Epoch [55/100], Train Loss: 2.1507, Validation Loss: 2.2567\n",
      "Epoch [56/100], Train Loss: 2.1494, Validation Loss: 2.2442\n",
      "Epoch [57/100], Train Loss: 2.1457, Validation Loss: 2.2624\n",
      "Epoch [58/100], Train Loss: 2.1691, Validation Loss: 2.2643\n",
      "Epoch [59/100], Train Loss: 2.1724, Validation Loss: 2.2944\n",
      "Epoch [60/100], Train Loss: 2.1721, Validation Loss: 2.2763\n",
      "Epoch [61/100], Train Loss: 2.1782, Validation Loss: 2.2512\n",
      "Epoch [62/100], Train Loss: 2.1808, Validation Loss: 2.3074\n",
      "Epoch [63/100], Train Loss: 2.1876, Validation Loss: 2.2780\n",
      "Epoch [64/100], Train Loss: 2.1730, Validation Loss: 2.2649\n",
      "Epoch [65/100], Train Loss: 2.1806, Validation Loss: 2.2572\n",
      "Epoch [66/100], Train Loss: 2.1718, Validation Loss: 2.2618\n",
      "Epoch [67/100], Train Loss: 2.1557, Validation Loss: 2.2835\n",
      "Epoch [68/100], Train Loss: 2.1824, Validation Loss: 2.2620\n",
      "Epoch [69/100], Train Loss: 2.1584, Validation Loss: 2.2688\n",
      "Epoch [70/100], Train Loss: 2.1704, Validation Loss: 2.2949\n",
      "Epoch [71/100], Train Loss: 2.1636, Validation Loss: 2.2625\n",
      "Epoch [72/100], Train Loss: 2.1615, Validation Loss: 2.3015\n",
      "Epoch [73/100], Train Loss: 2.1826, Validation Loss: 2.3175\n",
      "Epoch [74/100], Train Loss: 2.2052, Validation Loss: 2.2914\n",
      "Epoch [75/100], Train Loss: 2.1861, Validation Loss: 2.3068\n",
      "Epoch [76/100], Train Loss: 2.1752, Validation Loss: 2.3016\n",
      "Epoch [77/100], Train Loss: 2.1672, Validation Loss: 2.3122\n",
      "Epoch [78/100], Train Loss: 2.1656, Validation Loss: 2.2828\n",
      "Epoch [79/100], Train Loss: 2.1750, Validation Loss: 2.2792\n",
      "Epoch [80/100], Train Loss: 2.1822, Validation Loss: 2.3335\n",
      "Epoch [81/100], Train Loss: 2.1931, Validation Loss: 2.3051\n",
      "Epoch [82/100], Train Loss: 2.1978, Validation Loss: 2.3376\n",
      "Epoch [83/100], Train Loss: 2.1894, Validation Loss: 2.3093\n",
      "Epoch [84/100], Train Loss: 2.1837, Validation Loss: 2.3241\n",
      "Epoch [85/100], Train Loss: 2.1716, Validation Loss: 2.3129\n",
      "Epoch [86/100], Train Loss: 2.1749, Validation Loss: 2.2977\n",
      "Epoch [87/100], Train Loss: 2.1982, Validation Loss: 2.2961\n",
      "Epoch [88/100], Train Loss: 2.1884, Validation Loss: 2.3018\n",
      "Epoch [89/100], Train Loss: 2.1639, Validation Loss: 2.2702\n",
      "Epoch [90/100], Train Loss: 2.1534, Validation Loss: 2.2836\n",
      "Epoch [91/100], Train Loss: 2.1625, Validation Loss: 2.2853\n",
      "Epoch [92/100], Train Loss: 2.1674, Validation Loss: 2.2815\n",
      "Epoch [93/100], Train Loss: 2.1569, Validation Loss: 2.2831\n",
      "Epoch [94/100], Train Loss: 2.1561, Validation Loss: 2.3001\n",
      "Epoch [95/100], Train Loss: 2.1579, Validation Loss: 2.2916\n",
      "Epoch [96/100], Train Loss: 2.1780, Validation Loss: 2.3014\n",
      "Epoch [97/100], Train Loss: 2.1584, Validation Loss: 2.2764\n",
      "Epoch [98/100], Train Loss: 2.1635, Validation Loss: 2.2854\n",
      "Epoch [99/100], Train Loss: 2.1586, Validation Loss: 2.2813\n",
      "Epoch [100/100], Train Loss: 2.1695, Validation Loss: 2.3329\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.2914\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(model, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(model, start_token, length=100):\n",
    "    model.eval() \n",
    "    generated_sequence = [start_token]\n",
    "    input_token = start_token\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            input_ix = torch.tensor([[chr_to_idx[input_token]]], dtype=torch.long).to(device)\n",
    "            output = model(input_ix)\n",
    "            #print(output.shape)\n",
    "            \n",
    "            _, predicted_ix = torch.max(output, 1)\n",
    "            predicted_token = idx_to_chr[predicted_ix.item()]\n",
    "            \n",
    "            generated_sequence.append(predicted_token)\n",
    "            input_token = predicted_token\n",
    "\n",
    "    return ' '.join(generated_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sequence(model, \"m\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10112334787845612"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(-torch.tensor(test_loss)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
